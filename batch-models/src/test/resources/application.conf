application.env="local"
telemetry.version="2.1"
default.parallelization=10
spark_output_temp_dir="/tmp/"

# the below urls should be https - do not update them
lp.url="https://dev.ekstep.in/api/learning"
lp.path="/api/learning/v2/content/"

default.consumption.app.id="genie"
default.channel.id="in.sunbird"
default.creation.app.id="portal"

lp.contentmodel.versionkey=jd5ECm/o0BXwQCe8PfZY1NoUkB9HN41QjA80p22MKyRIcP5RW4qHw8sZztCzv87M

# Test Configurations
cassandra.service.embedded.enable=true
cassandra.cql_path="../scripts/database/data.cql"
cassandra.service.embedded.connection.port=9142
cassandra.keyspace_prefix="local_"
cassandra.hierarchy_store_prefix="dev_"

spark.cassandra.connection.host=127.0.0.1

# Slack Configurations
monitor.notification.channel = "testing"
monitor.notification.name = "dp-monitor"
# monitor.notification.slack = false

# DataExhaust configuration
data_exhaust {
	save_config {
		save_type="local"
		bucket="ekstep-dev-data-store"
		prefix="/tmp/data-exhaust/"
		public_s3_url="s3://"
		local_path="/tmp/data-exhaust-package/"
	}
	delete_source: "false"
	package.enable: "false"
}

cloud_storage_type="local"
storage.key.config="azure_storage_key"
storage.secret.config="azure_storage_secret"
reports.storage.key.config="reports_azure_storage_key"
reports.storage.secret.config="reports_azure_storage_secret"

service.search.url="https://dev.sunbirded.org/action/composite"
service.search.path="/v3/search"

#service.search.url="http://11.2.4.16:9000"

elasticsearch.service.endpoint="http://localhost:9200"
elasticsearch.index.compositesearch.name="compositesearch"

## Reports - Global config
cloud.container.reports="reports"

# course metrics container in azure
es.host="http://localhost"
es.composite.host="localhost"
es.port="9200"
course.metrics.es.alias="cbatchstats"
course.metrics.es.index.cbatchstats.prefix="cbatchstats-"
course.metrics.es.index.cbatch="cbatch"
course.metrics.cassandra.sunbirdKeyspace="sunbird"
course.metrics.cassandra.sunbirdCoursesKeyspace="sunbird_courses"
admin.reports.cloud.container="reports"
# Folder names within container
course.metrics.cloud.objectKey="src/test/resources/reports/"
assessment.metrics.cloud.objectKey="src/test/resources/assessment-reports/"
admin.metrics.cloud.objectKey="src/test/resources/admin-user-reports/"

# End of report folder names
course.metrics.cassandra.input.consistency="QUORUM"
assessment.metrics.cassandra.input.consistency="QUORUM"
assessment.metrics.content.index="compositesearch"
assessment.metrics.es.alias="cbatch-assessment"
assessment.metrics.es.index.prefix="cbatch-assessment-"
course.upload.reports.enabled=true
course.es.index.enabled=true
assessment.metrics.bestscore.report=true // BestScore or Latst Updated Score
assessment.metrics.supported.contenttype="SelfAssess"
spark.sql.caseSensitive=true

reports_storage_key="test"
reports_storage_secret="test"
# for only testing uploads to blob store
azure_storage_key=""
azure_storage_secret=""

# content rating configurations
druid.sql.host="http://localhost:8082/druid/v2/sql/"
druid.unique.content.query="{\"query\":\"SELECT DISTINCT \\\"object_id\\\" AS \\\"Id\\\"\\nFROM \\\"druid\\\".\\\"summary-events\\\" WHERE \\\"__time\\\"  BETWEEN TIMESTAMP '%s' AND TIMESTAMP '%s'\"}"
druid.content.rating.query="{\"query\":\"SELECT \\\"object_id\\\" AS contentId, COUNT(*) AS \\\"totalRatingsCount\\\", SUM(edata_rating) AS \\\"Total Ratings\\\", SUM(edata_rating)/COUNT(*) AS \\\"averageRating\\\" FROM \\\"druid\\\".\\\"telemetry-feedback-events\\\" WHERE \\\"eid\\\" = 'FEEDBACK' GROUP BY \\\"object_id\\\"\"}"
lp.system.update.base.url="http://localhost:8080/learning-service/system/v3/content/update"

druid = {
	hosts = "localhost:8082"
	secure = false
	url = "/druid/v2/"
	datasource = "summary-events"
	response-parsing-timeout = 300000
}
druid.rollup.host="localhost"
druid.rollup.port=8082

druid.query.wait.time.mins=2
druid.report.upload.wait.time.mins=1
druid.scan.batch.size=100000
druid.scan.batch.bytes=2000000
druid.query.batch.buffer=1
#Experiment Configuration
user.search.api.url = "http://localhost:9000/private/user/v1/search"
user.search.limit = 10000

spark.memory_fraction=0.3
spark.storage_fraction=0.5
spark.driver_memory=1g

// Metric event config
metric.producer.id="pipeline.monitoring"
metric.producer.pid="dataproduct.metrics"
push.metrics.kafka=false
metric.kafka.broker="localhost:9092"
metric.kafka.topic="metric"

//Postgres Config
postgres.db="postgres"
postgres.url="jdbc:postgresql://localhost:65124/"
postgres.user="postgres"
postgres.pass="postgres"
druid.content.consumption.query="{\"query\":\"SELECT COUNT(*) as \\\"play_sessions_count\\\", SUM(total_time_spent) as \\\"total_time_spent\\\", dimensions_pdata_id, object_id\\nFROM \\\"summary-events\\\"\\nWHERE \\\"dimensions_mode\\\" = 'play' AND \\\"dimensions_type\\\" ='content'\\nGROUP BY object_id, dimensions_pdata_id\"}"
// TPD Configurations
org.search.api.url="https://dev.sunbirded.org/api"
org.search.api.path="/org/v1/search"
druid.host="http://localhost:8082/druid/v2"
elasticsearch.index.coursebatch.name="course-batch"



location.search.url="https://localhost:9000/v1/location/search"
location.search.token="Token"

location.search.request="{\"request\": {\"filters\": {\"type\" :[\"state\",\"district\"]},\"limit\" : 10000}}"

druid.state.lookup.url = "http://localhost:8081/druid/coordinator/v1/lookups/config/__default/stateSlugLookup"

druid.report.default.storage="azure"
druid.report.date.format="yyyy-MM-dd"
push.metrics.kafka=true