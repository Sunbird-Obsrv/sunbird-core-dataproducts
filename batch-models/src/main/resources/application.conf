application.env="local"
telemetry.version="2.1"
default.parallelization=10
spark_output_temp_dir="/tmp/"

# the below urls should be https - do not update them
lp.url="https://dev.ekstep.in/api/learning"
lp.path="/api/learning/v2/content/"


# Content to vec configurations
content2vec_scripts_path="../../platform-scripts/python/main/vidyavaani"
pc_files_cache="file"
pc_dispatch_params="""{"bucket":""}"""
pc_files_prefix="/tmp/data-sets/"

metrics.dispatch.to="file"
metrics.dispatch.params="""{"bucket":""}"""
metrics.consumption.dataset.id="/tmp/data-sets/eks-consumption-metrics/"
metrics.creation.dataset.id="/tmp/data-sets/eks-creation-metrics/"

default.consumption.app.id="genie"
default.channel.id="in.ekstep"
default.creation.app.id="portal"

lp.contentmodel.versionkey=jd5ECm/o0BXwQCe8PfZY1NoUkB9HN41QjA80p22MKyRIcP5RW4qHw8sZztCzv87M

reactiveinflux {
  url = "http://localhost:8086/"
  spark {
    batchSize = 1000
  }
  database = "business_metrics_test_db"
  awaitatmost = "60"
}

# Neo4j
neo4j.bolt.url="bolt://localhost:7687"
neo4j.bolt.user="neo4j"
neo4j.bolt.password="@n@lytic5"


# Test Configurations
graph.service.embedded.enable=true
graph.service.embedded.dbpath="/tmp/graph.db/"
graph.content.limit="100"
cassandra.service.embedded.enable=true
cassandra.cql_path="../../platform-scripts/database/data.cql"
cassandra.service.embedded.connection.port=9142
cassandra.keyspace_prefix="local_"
cassandra.hierarchy_store_prefix="dev_"

spark.cassandra.connection.host=127.0.0.1

# Slack Configurations
monitor.notification.channel = "testing"
monitor.notification.name = "dp-monitor"
monitor.notification.slack = false

# DataExhaust configuration
data_exhaust {
	save_config {
		save_type="local"
		bucket="ekstep-dev-data-store"
		prefix="/tmp/data-exhaust/"
		public_s3_url="s3://"
		local_path="/tmp/data-exhaust-package/"
	}
	delete_source: "false"
	package.enable: "false"
}

cloud_storage_type="azure"

service.search.url="https://dev.open-sunbird.org/api/composite"
service.search.path="/v1/search"

## Reports - Global config
cloud.container.reports="reports"

# course metrics container in azure
es.host="http://localhost"
es.composite.host="localhost"
es.port="9200"
es.scroll.size = 1000
course.metrics.es.alias="cbatchstats"
course.metrics.es.index.cbatchstats.prefix="cbatchstats-"
course.metrics.es.index.cbatch="cbatch"
course.metrics.cassandra.sunbirdKeyspace="sunbird"
course.metrics.cassandra.sunbirdCoursesKeyspace="sunbird_courses"
admin.reports.cloud.container="reports"
# Folder names within container
course.metrics.cloud.objectKey="reports/"
assessment.metrics.cloud.objectKey="assessment-reports/"
admin.metrics.cloud.objectKey="public/admin-user-reports/"
admin.metrics.temp.dir="/tmp/admin-user-reports"
# End of report folder names
course.metrics.temp.dir="/tmp/course-reports"
course.metrics.cassandra.input.consistency="QUORUM"
assessment.metrics.temp.dir="/Users/admin/assessment-report"
assessment.metrics.cassandra.input.consistency="QUORUM"
assessment.metrics.content.index="compositesearch"
assessment.metrics.es.alias="cbatch-assessment"
assessment.metrics.es.index.prefix="cbatch-assessment-"
course.upload.reports.enabled=true
course.es.index.enabled=true
assessment.metrics.bestscore.report=true // BestScore or Latst Updated Score	
assessment.metrics.supported.contenttype="SelfAssess"

reports_azure_storage_key=""
reports_azure_storage_secret=""
# for only testing uploads to blob store
azure_storage_key=""
azure_storage_secret=""

# content rating configurations
druid.sql.host="http://localhost:8082/druid/v2/sql/"
druid.unique.content.query="{\"query\":\"SELECT DISTINCT \\\"object_id\\\" AS \\\"Id\\\"\\nFROM \\\"druid\\\".\\\"telemetry-events\\\" WHERE \\\"eid\\\" = 'FEEDBACK' AND \\\"__time\\\" BETWEEN TIMESTAMP '%s' AND TIMESTAMP '%s' \"}"
druid.content.rating.query="{\"query\": \"SELECT \\\"object_id\\\" AS ContentId, COUNT(*) AS \\\"Number of Ratings\\\", SUM(edata_rating) AS \\\"Total Ratings\\\", SUM(edata_rating)/COUNT(*) AS \\\"AverageRating\\\" FROM \\\"druid\\\".\\\"telemetry-events\\\" WHERE \\\"eid\\\" = 'FEEDBACK' GROUP BY \\\"object_id\\\"\"}"
lp.system.update.base.url="http://localhost:8080/learning-service/system/v3/content/update"

druid = {
	hosts = "localhost:8082"
	secure = false
	url = "/druid/v2/"
	datasource = "summary-events"
	response-parsing-timeout = 300000
}
druid.query.wait.time.mins=1
druid.report.upload.wait.time.mins=1

#Experiment Configuration
user.search.api.url = "http://localhost:9000/private/user/v1/search"
user.search.limit = 10000

spark.memory_fraction=0.3
spark.storage_fraction=0.5
spark.driver_memory=1g

# pipeline auditing
//druid.pipeline_metrics.audit.query="{\"query\":\"SELECT \\\"job-name\\\", SUM(\\\"success-message-count\\\") AS \\\"success-message-count\\\", SUM(\\\"failed-message-count\\\") AS \\\"failed-message-count\\\", SUM(\\\"duplicate-event-count\\\") AS \\\"duplicate-event-count\\\", SUM(\\\"batch-success-count\\\") AS \\\"batch-success-count\\\", SUM(\\\"batch-error-count\\\") AS \\\"batch-error-count\\\", SUM(\\\"primary-route-success-count\\\") AS \\\"primary-route-success-count\\\", SUM(\\\"secondary-route-success-count\\\") AS \\\"secondary-route-success-count\\\" FROM \\\"druid\\\".\\\"pipeline-metrics\\\" WHERE \\\"job-name\\\" IN (%s) AND \\\"__time\\\" BETWEEN TIMESTAMP '%s' AND TIMESTAMP '%s' GROUP BY \\\"job-name\\\" \"}"
//druid.datasource.count.query="{ \"query\": \"SELECT COUNT(*) AS \\\"total\\\" FROM \\\"druid\\\".\\\"%s\\\" WHERE TIME_FORMAT(MILLIS_TO_TIMESTAMP(\\\"syncts\\\"), 'yyyy-MM-dd HH:mm:ss.SSS', 'Asia/Kolkata') BETWEEN TIMESTAMP '%s' AND '%s' AND  \\\"__time\\\" BETWEEN TIMESTAMP '%s' AND TIMESTAMP '%s'\" }"

//Postgres Config
postgres.db="postgres"
postgres.url="jdbc:postgresql://localhost:65124/"
postgres.user="postgres"
postgres.pass="postgres"